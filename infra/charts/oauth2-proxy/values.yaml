# Default values for keycloak.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# ---------------------------------------
# Global variables
# ---------------------------------------
nameOverride: ""
namespaceOverride: ""
fullnameOverride: ""
restartPolicy: Always

# ---------------------------------------------
# Variable used in hpa template and deployment
# ---------------------------------------------
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 3
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

# Force the target Kubernetes version (it uses Helm `.Capabilities` if not set).
# This is especially useful for `helm template` as capabilities are always empty
# due to the fact that it doesn't query an actual cluster
kubeVersion:

# ---------------------------------------
# Variable used in Deployment template
# ---------------------------------------

replicaCount: 1
# Additional Pod annotations
podAnnotations: {}
# Additional Pod labels
podLabels: {}
deploymentAnnotations: {}
revisionHistoryLimit: 10
# Image pull secrets for the Pod
imagePullSecrets: []
# SecurityContext for the entire Pod. Every container running in the Pod will inherit this SecurityContext. This might be relevant when other components of the environment inject additional containers into running Pods (service meshes are the most prominent example for this)
podSecurityContext:
  fsGroup: 1000
# Configure Kubernetes security context for container
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/security-context/
securityContext:
  enabled: true
  runAsUser: 1000
  # runAsNonRoot: true
  # allowPrivilegeEscalation: false

## PodDisruptionBudget settings
## ref: https://kubernetes.io/docs/concepts/workloads/pods/disruptions/
podDisruptionBudget:
  enabled: true
  minAvailable: 1
# whether to use http or https
httpScheme: https


image:
  repository: "quay.io/oauth2-proxy/oauth2-proxy"
  pullPolicy: IfNotPresent
  # Optionally specify an array of imagePullSecrets.
  # Secrets must be manually created in the namespace.
  # ref: https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
  # imagePullSecrets:
  # - name: myRegistryKeySecretName
  # Set a custom containerPort if required.
  # This will default to 4180 if this value is not set and the httpScheme set to http
  # This will default to 4443 if this value is not set and the httpScheme set to https
  # containerPort: 4180

extraArgs: {}
extraEnv: []
# -- Custom labels to add into metadata
customLabels: {}
service:
  type: ClusterIP
  # when service.type is ClusterIP ...
  # clusterIP: 192.0.2.20
  # when service.type is LoadBalancer ...
  # loadBalancerIP: 198.51.100.40
  # loadBalancerSourceRanges: 203.0.113.0/24
  # when service.type is NodePort ...
  # nodePort: 80
  portNumber: 4120
  # Protocol set on the service
  appProtocol: https
  annotations: {}
  # foo.io/bar: "true"

extraVolumes: []
# - name: ca-bundle-cert
#   secret:
#     secretName: <secret-name>

extraVolumeMounts: []
# - mountPath: /etc/ssl/certs/
#   name: ca-bundle-cert

# Additional containers to be added to the pod.
extraContainers: []
#  - name: my-sidecar
#    image: nginx:latest

# Whether to use secrets instead of environment values for setting up OAUTH2_PROXY variables
proxyVarsAsSecrets: true

# Additionally authenticate against a htpasswd file. Entries must be created with "htpasswd -B" for bcrypt encryption.
# Alternatively supply an existing secret which contains the required information.
htpasswdFile:
  enabled: false
  existingSecret: ""
  entries: [ ]
  # One row for each user
  # example:
  # entries:
  #  - testuser:$2y$05$gY6dgXqjuzFhwdhsiFe7seM9q9Tile4Y3E.CBpAZJffkeiLaC21Gy


# Configure the session storage type, between cookie and redis
sessionStorage:
  # Can be one of the supported session storage cookie|redis
  type: cookie
  redis:
  # Name of the Kubernetes secret containing the redis & redis sentinel password values (see also `sessionStorage.redis.passwordKey`)
    existingSecret: ""
    # Redis password value. Applicable for all Redis configurations. Taken from redis subchart secret if not set. `sessionStorage.redis.existingSecret` takes precedence
    password: ""
    # Key of the Kubernetes secret data containing the redis password value
    passwordKey: "redis-password"
    # Can be one of standalone|cluster|sentinel
    clientType: "standalone"
    standalone:
      # URL of redis standalone server for redis session storage (e.g. `redis://HOST[:PORT]`). Automatically generated if not set
      connectionUrl: ""
    cluster:
      # List of Redis cluster connection URLs (e.g. `["redis://127.0.0.1:8000", "redis://127.0.0.1:8000"]`)
      connectionUrls: [ ]
    sentinel:
      # Name of the Kubernetes secret containing the redis sentinel password value (see also `sessionStorage.redis.sentinel.passwordKey`). Default: `sessionStorage.redis.existingSecret`
      existingSecret: ""
      # Redis sentinel password. Used only for sentinel connection; any redis node passwords need to use `sessionStorage.redis.password`
      password: ""
      # Key of the Kubernetes secret data containing the redis sentinel password value
      passwordKey: "redis-sentinel-password"
      # Redis sentinel master name
      masterName: ""
      # List of Redis sentinel connection URLs (e.g. `["redis://127.0.0.1:8000", "redis://127.0.0.1:8000"]`)
      connectionUrls: [ ]
# Enables and configure the automatic deployment of the redis subchart
redis:
  # provision an instance of the redis sub-chart
  enabled: false
  # Redis specific helm chart settings, please see:
  # https://github.com/bitnami/charts/tree/master/bitnami/redis#parameters
  # redisPort: 6379
  # cluster:
  #   enabled: false
  #   slaveCount: 1

# Enables apiVersion deprecation checks
checkDeprecation: true

metrics:
  # Enable Prometheus metrics endpoint
  enabled: true
  # Serve Prometheus metrics on this port
  port: 44180
  # when service.type is NodePort ...
  # nodePort: 44180
  # Protocol set on the service for the metrics port
  service:
    appProtocol: http
  servicemonitor:
    # Enable Prometheus Operator ServiceMonitor
    enabled: false
    # Define the namespace where to deploy the ServiceMonitor resource
    namespace: ""
    # Prometheus Instance definition
    prometheusInstance: default
    # Prometheus scrape interval
    interval: 60s
    # Prometheus scrape timeout
    scrapeTimeout: 30s
    # Add custom labels to the ServiceMonitor resource
    labels: {}

# Extra K8s manifests to deploy
extraObjects: []
  # - apiVersion: secrets-store.csi.x-k8s.io/v1
  #   kind: SecretProviderClass
  #   metadata:
  #   name: oauth2-proxy-secrets-store
  #   spec:
  #   provider: aws
  #   parameters:
  #     objects: |
  #       - objectName: "oauth2-proxy"
  #       objectType: "secretsmanager"
  #       jmesPath:
  #           - path: "client_id"
  #           objectAlias: "client-id"
  #           - path: "client_secret"
  #           objectAlias: "client-secret"
  #           - path: "cookie_secret"
  #           objectAlias: "cookie-secret"
  #   secretObjects:
  #   - data:
  #     - key: client-id
  #       objectName: client-id
  #       - key: client-secret
  #       objectName: client-secret
  #       - key: cookie-secret
  #       objectName: cookie-secret
  #     secretName: oauth2-proxy-secrets-store
  #     type: Opaque

priorityClassName: ""

# Host aliases, useful when working "on premise" where (public) DNS resolver does not know about my hosts.
hostAlias:
  enabled: false
  # ip: "10.xxx.xxx.xxx"
  # hostname: "auth.example.com"

# Define which port will be used in the containers
containerPort: 4180
# Configure Kubernetes liveness and readiness probes.
# Ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
# Disable both when deploying with Istio 1.0 mTLS. https://istio.io/help/faq/security/#k8s-health-checks
livenessProbe:
  enabled: true
  initialDelaySeconds: 0
  timeoutSeconds: 1
readinessProbe:
  enabled: true
  initialDelaySeconds: 0
  timeoutSeconds: 5
  periodSeconds: 10
  successThreshold: 1
# Pod resource requests and limits
resources: {}
  # requests:
  #   cpu: "500m"
  #   memory: "1024Mi"
  # limits:
  #   cpu: "500m"
  #   memory: "1024Mi"
# Node labels for Pod assignment
nodeSelector: {}
# Pod affinity
affinity: {}
# Node taints to tolerate
tolerations: []

# ---------------------------------------
# Variable group used in ingress template
# ---------------------------------------
ingress:
  enabled: true
  className: ""
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-staging
  hosts:
    - host: identity-oauth2-proxy.local.eoepca.org
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: oauth2-proxy-tls-certificate
      hosts:
        - identity-oauth2-proxy.local.eoepca.org

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:
  automountServiceAccountToken: true



alphaConfig:
  enabled: false
  # Add config annotations
  annotations: {}
  # Arbitrary configuration data to append to the server section
  serverConfigData: {}
  # Arbitrary configuration data to append to the metrics section
  metricsConfigData: {}
  # Arbitrary configuration data to append
  configData: {}
  # Arbitrary configuration to append
  # This is treated as a Go template and rendered with the root context
  configFile: ""
  # Use an existing config map (see configmap-alpha.yaml for required fields)
  existingConfig: ~



# To authorize individual email addresses
# That is part of extraArgs but since this needs special treatment we need to do a separate section
authenticatedEmailsFile:
  enabled: false
  # Defines how the email addresses file will be projected, via a configmap or secret
  persistence: configmap
  # template is the name of the configmap what contains the email user list but has been configured without this chart.
  # It's a simpler way to maintain only one configmap (user list) instead changing it for each oauth2-proxy service.
  # Be aware the value name in the extern config map in data needs to be named to "restricted_user_access" or to the
  # provided value in restrictedUserAccessKey field.
  emailUserListTemplate: ""
  # The configmap/secret key under which the list of email access is stored
  # Defaults to "restricted_user_access" if not filled-in, but can be overridden to allow flexibility
  restrictedUserAccessKey: ""
  # One email per line
  # example:
  # restricted_access: |-
  #   name1@domain
  #   name2@domain
  # If you override the config with restricted_access it will configure a user list within this chart what takes care of the
  # config map resource.
  restricted_access: ""
  annotations: {}
  # helm.sh/resource-policy: keep


config:
  # Add config annotations
  annotations: {}
  clientID: "oauth2-proxy"
  clientSecret: "secret"
  cookieSecret: "JxJZrvNo4z1VJIiDMDGJe3q4TjmpVxMNHziW97J9UAo="
  # The name of the cookie that oauth2-proxy will create
  # If left empty, it will default to the release name
  cookieName: ""
  google: {}
  # Use an existing config map (see configmap.yaml for required fields)
  existingConfig: ~
  configFile: |-
    http_address = "0.0.0.0:4180"
    cookie_secure = true
    cookie_secret = "JxJZrvNo4z1VJIiDMDGJe3q4TjmpVxMNHziW97J9UAo="
    upstreams = ["https://dummy-service.develop.eoepca.org"]
    cookie_domains = [".eoepca.org"] # Required so cookie can be read on all subdomains.
    whitelist_domains = [".eoepca.org"] # Required to allow redirection back to original requested target.
    pass_access_token = true
    set_xauthrequest = true
    # allow accounts to sign-in with empty emails
    # https://github.com/oauth2-proxy/oauth2-proxy/issues/769#issuecomment-808916298
    email_domains = ["*"]
    insecure_oidc_allow_unverified_email = true
    oidc_email_claim = "sub"
    # keycloak provider
    client_id = "oauth2-proxy"
    client_secret = "secret"
    redirect_url = "https://identity-proxy.local.eoepca.org/oauth2/callback"
    # in this case oauth2-proxy is going to visit
    # https://identity-keycloak.local.eoepca.org/realms/demo/.well-known/openid-configuration for configuration
    oidc_issuer_url = "https://identity-keycloak.local.eoepca.org/realms/demo"
    provider = "oidc"
    provider_display_name = "Keycloak"
    skip_provider_button = true
    skip_jwt_bearer_tokens = true
